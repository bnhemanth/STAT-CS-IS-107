{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "<div style=\"color: #DD3403; font-size: 60%\">Data Science DISCOVERY MicroProject</div>\n",
    "<span style=\"\">AI versus Human: Response Analysis</span>\n",
    "<div style=\"font-size: 60%;\"><a href=\"https://discovery.cs.illinois.edu/microproject/ai-vs-human-response-analysis/\">https://discovery.cs.illinois.edu/microproject/ai-vs-human-response-analysis/</a></div>\n",
    "</h1>\n",
    "\n",
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Source: Hugging Face\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) is a company that has developed a platform for natural language processing (NLP) applications. They have created and shared a large collection of pre-trained models, datasets, and learning resources which are open-source and available for the public to use.\n",
    "\n",
    "Hello-SimpleAI is a small group of researchers (PhD students and engineers) that released the [HC3 dataset](https://arxiv.org/abs/2301.07597). This dataset compares human and AI responses to the same question.\n",
    "\n",
    "They also have a public [API](https://huggingface.co/datasets/Hello-SimpleAI/HC3/viewer/all/train) that allows you to access the data with the requests library."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "For this MicroProject, we have provided the first 100 samples of the `Hello-SampleAI` dataset that was gathered from the following URL: https://datasets-server.huggingface.co/rows?dataset=Hello-SimpleAI%2FHC3&config=all&split=train&offset=0&length=100\n",
    "\n",
    "This data has been cleaned up and provided to you as `huggingface-hello-sampleAI-100.csv`.  Load this dataset as a DataFrame `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Data Import\n",
    "# - This read-only cell contains a \"checkpoint\" for this section of the MicroProejct and verifies you are on the right track.\n",
    "# - If this cell results in a celebration message, you PASSED all test cases!\n",
    "# - If this cell results in any errors, check you previous cells, make changes, and RE-RUN your code and then this cell.\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "\n",
    "df = df.replace(r'\\r\\n', '\\n', regex=True)\n",
    "\n",
    "assert(\"df\" in vars())\n",
    "assert(len(df) == 100)\n",
    "assert(\"How come I have to pay for Foxtel and get ads , when things like youtube are free because they play ads . Should n't Foxtel be you pay and you do nt get ads , or it should be free and you get ads . Please explain like I'm five.\" in df[\"question\"].values)\n",
    "assert(\"It's important to remember that Japan is a country with a rich and diverse culture, and like any other country, it has a range of cultural practices and expressions. While it is true that Japan has a reputation for being a more traditional and conservative culture, it is also home to a vibrant and creative media industry. \\nThe Japanese media industry is known for producing a wide variety of content, including video games, TV shows, and music, that often incorporates elements of Japanese culture and history. This can include things like traditional Japanese instruments and themes, as well as more modern and unconventional elements. \\nIt's also worth noting that different parts of Japanese culture can have different levels of conservatism. For example, some aspects of Japanese culture, such as traditional family structure or gender roles, may be more conservative, while other parts of Japanese culture, such as the media industry, may be more open to experimentation and innovation. \\nOverall, it's important to remember that Japan is a complex and multifaceted culture, and it's not fair to generalize or stereotype all aspects of Japanese culture based on a few specific examples.\" in df[\"chatgpt_answer\"].values)\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1: Response Length Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puzzle 1.1: Getting the average length of the human and AI response"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "First, let's compare the average length of the human response to the average length of an AI response. To start off add two new columns to the DataFrame that contain the length of each response.\n",
    "\n",
    "When working with columns that contain `string` data, we can use the general syntax:\n",
    "\n",
    "```\n",
    "df[\"column\"].str.string_function()\n",
    "```\n",
    "\n",
    "Where `string_function()` is any of the [string accessor methods](https://pandas.pydata.org/docs/reference/series.html#api-series-str).  Specifically, one such method is:\n",
    "\n",
    "> ...\n",
    "> `Series.str.len()`: Compute the length of each element in the Series/Index.\n",
    "> ...\n",
    "\n",
    "To begin our analysis of the length of each response, add two new columns to the DataFrame:\n",
    "- `human_answer_len`, containing the length (in characters) of the human response\n",
    "- `chatgpt_answer_len`, containing the length (in characters) of the ChatGPT response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puzzle 1.2: Compute the Average Answer Lengths\n",
    "\n",
    "Now that we have the lengths of our responses, save the averages length of the responses in the variables `human_avg` and `ai_avg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average response length for a human response:\n",
    "human_avg = ...\n",
    "human_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average response length for a ChatGPT response:\n",
    "ai_avg = ...\n",
    "ai_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 1: Response Length Analysis (Puzzles 1.1 and 1.2)\n",
    "\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"human_avg\" in vars())\n",
    "assert(\"ai_avg\" in vars())\n",
    "assert('human_answer_len' in df.columns)\n",
    "assert('chatgpt_answer_len' in df.columns)\n",
    "assert(876 in df[\"human_answer_len\"].values and 930 in df[\"chatgpt_answer_len\"].values)\n",
    "assert((human_avg * 390)/30 == 9488.44)\n",
    "assert(((ai_avg * 444)/30) + 86 == 16347.796)\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puzzle 1.3: Testing if the difference in length is statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "To determine if the difference between the means is statistically significant, we're going to conduct a t-test. Let's first state our null and alternative hypotheses.\n",
    "\n",
    "> **Null hypothesis**: There is no significant difference between the means of the two response lengths.\n",
    "> \n",
    "> **Alternative hypothesis**: There is a significant difference between the means of the two response lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the [`scipy.stats.ttest_ind`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) function to get the t-stat (as `t_stat`) and p-value (as `p_value`).\n",
    "\n",
    "You can find the function documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a t-test:\n",
    "#    Syntax: t_stat, p_value = ttest_ind( ... )\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the t-statistic:\n",
    "t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the p-value:\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puzzle 1.4: Choosing Your Conclusion\n",
    "\n",
    "Using the calculated p-value and an alpha level of 0.05, we can determine if there is a significant difference between the two mean values.\n",
    "\n",
    "Un-comment exactly one conclusion based on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion = \"The p-value is greater than .05 so there is a significant difference between the means of the two response lengths.\"\n",
    "# conclusion = \"The p-value is greater than .05 so there is no significant difference between the means of the two response lengths.\"\n",
    "# conclusion = \"The p-value is less than .05 so there is a significant difference between the means of the two response lengths.\"\n",
    "# conclusion = \"The p-value is less than .05 so there is no significant difference between the means of the two response lengths.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 1: Response Length Analysis (Puzzles 1.3 and 1.4)\n",
    "\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"t_stat\" in vars())\n",
    "assert(\"p_value\" in vars())\n",
    "\n",
    "import math\n",
    "assert( math.isclose(t_stat * p_value, -0.005055524973109543) ), \"Check your t_stat and p_value.\"\n",
    "\n",
    "assert(len(conclusion) == 112), \"Your conclusion is incorrect.\"\n",
    "assert(conclusion.count(\"a\") == 5), \"Your conclusion is incorrect.\"\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2: Sentiment Analysis - Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puzzle 2.1: Find the Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "We are going to use the TextBlob library, to calculate a polarity and subjectivity score for each response. TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. \n",
    "\n",
    "\n",
    "Start off by installing this library.\n",
    "\n",
    "In the terminal, type the following: `python3 -m pip install textblob`\n",
    "\n",
    "Once it is installed, import the `TextBlob` library with the following import statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "The polarity score from TextBlob ranges from -1 to 1, where -1 is very negative, 0 is neutral, and 1 is very positive. Scroll to the \"Sentiment Analysis\" portion of this [documentation](https://textblob.readthedocs.io/en/dev/quickstart.html).\n",
    "\n",
    "Once you understand how to get the of a string using the library, complete the function `polarity` below that takes a string `s` as input and returns the `polarity` score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(s):\n",
    "  ...\n",
    "  return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's test your function to make sure it's working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the documentation, we expect a positive (above zero) polarity:\n",
    "polarity(\"Textblob is amazingly simple to use. What great fun!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A negative statement, we expect a negative (below zero) polarity:\n",
    "polarity(\"You will fail DISCOVERY if you do not take the final exam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neutral statement, we expect a zero polarity:\n",
    "polarity(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try your own sentence :)\n",
    "polarity(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puzzle 2.2: Finding the Polarity of Human Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Once you have defined a custom function, you can use it on every row of data by using:\n",
    "> ```python\n",
    "> df[\"column\"].apply( customFunctionName )\n",
    "> ```\n",
    "\n",
    "Note that `customFunctionName` does NOT contain the usual `(` `)`, you are just providing the name of the function and the `apply` function will internally call the function.\n",
    "\n",
    "Using the syntax above, create a new column named `human_polarity` that finds the polarity of the `human_answer` data.  (*Your syntax will be nearly identical to what is provided above.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"human_polarity\"] = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puzzle 2.3: Finding the Polarity of ChatGPT Answers\n",
    "\n",
    "Now, find the polarity of ChatGPT answers and save each polarity in the column `chatgpt_polarity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"chatgpt_polarity\"] = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 2: Sentiment Analysis - Polarity\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "\n",
    "assert('human_polarity' in df.columns)\n",
    "assert('chatgpt_polarity' in df.columns)\n",
    "\n",
    "import math\n",
    "assert(math.isclose( df[\"human_polarity\"].mean(), 0.07790771420489018 ))\n",
    "assert(math.isclose( df[\"chatgpt_polarity\"].std(), 0.11014935757814867 ))\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3: Sentiment Analysis - Subjectivity\n",
    "\n",
    "Using the same library and design as Part #2, create a function called `subjectivity` that returns the subjectivity score of a given string `s`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity(s):\n",
    "  ...\n",
    "  return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A highly subjective statement, expecting a score above 0.5:\n",
    "subjectivity(\"The best fruit is a kiwi and everyone else is wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A factual statement, expecting a score near 0:\n",
    "subjectivity(\"Winter is colder than summer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try your own :)\n",
    "subjectivity(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finding the Subjectivity of Human and ChatGPT Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add two new columns to the DataFrame, `human_subjectivity` and `chatgpt_subjectivity`, that stores the subjectivity of each answer for all of the questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"human_subjectivity\"] = ...\n",
    "df[\"chatgpt_subjectivity\"] = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 3: Sentiment Analysis - Subjectivity\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "\n",
    "import math\n",
    "\n",
    "assert('human_subjectivity' in df.columns)\n",
    "assert('chatgpt_subjectivity' in df.columns)\n",
    "assert( math.isclose( df[\"human_subjectivity\"].mean(), 0.4507534813188739 ) )\n",
    "assert( math.isclose( df[\"chatgpt_subjectivity\"].std(), 0.10025355258208316 ) )\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Puzzle 4: Testing if the Difference in Subjectivity is Statistically Significant"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "To determine if the difference between the subjectivity is statistically significant, we're going to conduct a t-test. Let's first state our null and alternative hypotheses.\n",
    "\n",
    "> **Null hypothesis**: There is no significant difference between the means of the human and ChatGPT subjectivity.\n",
    "> \n",
    "> **Alternative hypothesis**: There is a significant difference between the means of the human and ChatGPT subjectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the scipy.stats.ttest_ind function again to get the t-stat and p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a t-test:\n",
    "#    Syntax: t_stat, p_value = ttest_ind( ... )\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the calculated p-value and an alpha level of .05, we can determine if there is a significant difference between the two mean values. Un-comment the correct conclusion based on these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion = \"The p-value is LESS THAN .05 so there is a significant difference between the means of the two response subjectivity.\"\n",
    "# conclusion = \"The p-value is LESS THAN .05 so there is *NO* significant difference between the means of the two response subjectivity.\"\n",
    "# conclusion = \"The p-value is GREATER THAN .05 so there is a significant difference between the means of the two response subjectivity.\"\n",
    "# conclusion = \"The p-value is GREATER THAN .05 so there is *NO* significant difference between the means of the two response subjectivity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 4: Testing if the Difference in Subjectivity is Statistically Significant\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"t_stat\" in vars())\n",
    "assert(\"p_value\" in vars())\n",
    "assert(len(conclusion) == 117)\n",
    "assert(conclusion.count(\"a\") == 4)\n",
    "\n",
    "import math\n",
    "assert( math.isclose(t_stat * p_value, -0.031921405408979774) ), \"Check your t_stat and p_value.\"\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission\n",
    "\n",
    "You're almost done!  All you need to do is to commit your lab to GitHub and run the GitHub Actions Grader:\n",
    "\n",
    "1.  ⚠️ **Make certain to save your work.** ⚠️ To do this, go to **File => Save All**\n",
    "\n",
    "2.  After you have saved, exit this notebook and return to https://discovery.cs.illinois.edu/microproject/ai-vs-human-response-analysis/ and complete the section **\"Commit and Grade Your Notebook\"**.\n",
    "\n",
    "3. If you see a 100% grade result on your GitHub Action, you've completed this MicroProject! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
